<!DOCTYPE html>
<html>
<head>
    <meta charset='utf-8'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <title>Ivy Streams</title>
    <meta name='viewport' content='width=device-width, initial-scale=1'>

    <style>
        body {
            background: #0F2027;
            background: -webkit-linear-gradient(to right, #2C5364, #203A43, #0F2027);
            background: linear-gradient(to right, #2C5364, #203A43, #0F2027);
        }

        #join-btn {
            position: fixed;
            top: 50%;
            left: 50%;
            margin-top: -50px;
            margin-left: -100px;
            font-size: 18px;
            padding: 20px 40px;
        }

        #video-streams {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(500px, 1fr));
            height: 90vh;
            width: 1400px;
            margin: 0 auto;
        }

        .video-container {
            max-height: 100%;
            border: 2px solid black;
            background-color: #203A49;
            position: relative;
        }

        .video-player {
            height: 100%;
            width: 100%;
        }

        .prediction-box {
            position: absolute;
            bottom: 10px;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(0,0,0,0.6);
            color: #fff;
            padding: 6px 12px;
            border-radius: 6px;
        }

        button {
            border: none;
            background-color: cadetblue;
            color: #fff;
            padding: 10px 20px;
            font-size: 16px;
            margin: 2px;
            cursor: pointer;
        }

        #stream-controls {
            display: none;
            justify-content: center;
            margin-top: 0.5em;
        }

        @media screen and (max-width: 1400px) {
            #video-streams {
                grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                width: 95%;
            }
        }
    </style>
</head>
<body>

    <button id="join-btn">Join Stream</button>

    <div id="stream-wrapper">
        <div id="video-streams"></div>

        <div id="stream-controls">
            <button id="leave-btn">Leave Stream</button>
            <button id="mic-btn">Mic On</button>
            <button id="camera-btn">Camera On</button>
        </div>
    </div>

    <!-- Agora & MediaPipe + TF.js Scripts -->
    <script src="https://cdn.jsdelivr.net/npm/agora-rtc-sdk-ng/AgoraRTC_N.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>

    <script>
        const APP_ID = "0f3fde8ae17c4048bcfc8d69286bc851"; // Replace with your Agora App ID
        const CHANNEL = "test";
        const TOKEN = null;

        let client = AgoraRTC.createClient({ mode: "rtc", codec: "vp8" });
        let localTracks = [];
        let localUid = null;
        let micMuted = false;
        let camMuted = false;

        document.getElementById("join-btn").addEventListener("click", async () => {
            localUid = await client.join(APP_ID, CHANNEL, TOKEN, null);
            localTracks[0] = await AgoraRTC.createMicrophoneAudioTrack();
            localTracks[1] = await AgoraRTC.createCameraVideoTrack();

            let player = `<div class="video-container" id="user-container-${localUid}">
                <video class="video-player" autoplay playsinline id="user-${localUid}"></video>
            </div>`;
            document.getElementById("video-streams").insertAdjacentHTML("beforeend", player);
            localTracks[1].play(`user-${localUid}`);

            await client.publish(localTracks);
            document.getElementById("join-btn").style.display = "none";
            document.getElementById("stream-controls").style.display = "flex";

            startGestureRecognition(document.getElementById(`user-${localUid}`));
        });

        document.getElementById("leave-btn").addEventListener("click", async () => {
            for (track of localTracks) {
                track.stop();
                track.close();
            }
            await client.leave();
            document.getElementById("video-streams").innerHTML = "";
            document.getElementById("stream-controls").style.display = "none";
            document.getElementById("join-btn").style.display = "block";
        });

        document.getElementById("mic-btn").addEventListener("click", () => {
            micMuted = !micMuted;
            localTracks[0].setEnabled(!micMuted);
            document.getElementById("mic-btn").innerText = micMuted ? "Mic Off" : "Mic On";
        });

        document.getElementById("camera-btn").addEventListener("click", () => {
            camMuted = !camMuted;
            localTracks[1].setEnabled(!camMuted);
            document.getElementById("camera-btn").innerText = camMuted ? "Camera Off" : "Camera On";
        });

        client.on("user-published", async (user, mediaType) => {
            await client.subscribe(user, mediaType);

            if (mediaType === "video") {
                const remotePlayer = `<div class="video-container" id="user-container-${user.uid}">
                    <video class="video-player" autoplay playsinline id="user-${user.uid}"></video>
                </div>`;
                document.getElementById("video-streams").insertAdjacentHTML("beforeend", remotePlayer);
                user.videoTrack.play(`user-${user.uid}`);
            }

            if (mediaType === "audio") {
                user.audioTrack.play();
            }
        });

        client.on("user-unpublished", (user) => {
            document.getElementById(`user-container-${user.uid}`).remove();
        });

        // ========== Gesture Model ==========
        let model, labels, camera;
        const confidenceThreshold = 0.7;

        const hands = new Hands({
            locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`
        });

        hands.setOptions({
            maxNumHands: 1,
            modelComplexity: 1,
            minDetectionConfidence: 0.7,
            minTrackingConfidence: 0.7
        });

        async function loadModel() {
            model = await tf.loadLayersModel("model_js/model.json");
            labels = await (await fetch("model_js/labels.json")).json();
            console.log("Gesture model loaded.");
        }

        async function startGestureRecognition(videoElement) {
            await loadModel();

            const predictionBox = document.createElement("div");
            predictionBox.className = "prediction-box";
            predictionBox.innerText = "Detecting...";
            videoElement.parentElement.appendChild(predictionBox);

            camera = new Camera(videoElement, {
                onFrame: async () => {
                    await hands.send({ image: videoElement });
                },
                width: 640,
                height: 480
            });
            camera.start();

            hands.onResults((results) => {
                if (results.multiHandLandmarks?.length) {
                    const flat = results.multiHandLandmarks[0].flatMap(pt => [pt.x, pt.y, pt.z]);
                    const padded = flat.concat(new Array(188 - flat.length).fill(0));
                    const input = tf.tensor2d([padded]);
                    const prediction = model.predict(input).dataSync();
                    const maxIndex = prediction.indexOf(Math.max(...prediction));
                    const confidence = prediction[maxIndex];

                    predictionBox.innerText = confidence > confidenceThreshold
                        ? `Sign: ${labels[maxIndex]}`
                        : "Uncertain";
                    tf.dispose(input);
                } else {
                    predictionBox.innerText = "No hand detected";
                }
            });
        }
    </script>
</body>
</html>
