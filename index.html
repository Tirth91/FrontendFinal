<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Ivy Streams</title>

  <!-- Agora & AI -->
  <script src="https://download.agora.io/sdk/release/AgoraRTC_N.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>

  <!-- Icons -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <style>
    body {
      margin: 0;
      font-family: 'Segoe UI', sans-serif;
      background-color: #0d1117;
      color: white;
    }

    header {
      padding: 1rem;
      text-align: center;
      background-color: #161b22;
      font-size: 1.5rem;
      font-weight: bold;
    }

    #video-container {
      display: grid;
      grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
      gap: 12px;
      padding: 16px;
    }

    .video-box {
      position: relative;
      background-color: black;
      border-radius: 10px;
      overflow: hidden;
      box-shadow: 0 0 10px rgba(255, 255, 255, 0.1);
    }

    video {
      width: 100%;
      height: auto;
      object-fit: cover;
    }

    .prediction-box, .username {
      position: absolute;
      left: 50%;
      transform: translateX(-50%);
      background: rgba(0, 0, 0, 0.6);
      padding: 4px 10px;
      border-radius: 6px;
      font-size: 14px;
      pointer-events: none;
    }

    .prediction-box {
      bottom: 8px;
    }

    .username {
      top: 8px;
    }

    #controls {
      display: flex;
      justify-content: center;
      gap: 10px;
      padding: 1rem;
      background-color: #161b22;
    }

    .control-btn {
      background: #238636;
      color: white;
      border: none;
      padding: 10px 15px;
      border-radius: 6px;
      font-size: 1rem;
      cursor: pointer;
      display: flex;
      align-items: center;
      gap: 8px;
    }

    .control-btn:hover {
      background: #2ea043;
    }

    .danger {
      background: #da3633;
    }

    .danger:hover {
      background: #f85149;
    }
  </style>
</head>
<body>

  <header>Ivy Streams â€” Gesture Video Call</header>

  <div id="video-container"></div>

  <div id="controls">
    <button class="control-btn" onclick="joinCall()"><i class="fas fa-sign-in-alt"></i> Join</button>
    <button class="control-btn" onclick="toggleMic()"><i class="fas fa-microphone" id="mic-icon"></i> Mic</button>
    <button class="control-btn" onclick="toggleVideo()"><i class="fas fa-video" id="video-icon"></i> Video</button>
    <button class="control-btn danger" onclick="leaveCall()"><i class="fas fa-sign-out-alt"></i> Leave</button>
  </div>

  <script>
    const APP_ID = "0f3fde8ae17c4048bcfc8d69286bc851";
    const CHANNEL_NAME = "test";
    const TOKEN = null;

    let client, localVideoTrack, localAudioTrack;
    let model, labels;
    let camera;
    const confidenceThreshold = 0.7;
    let isMicOn = true;
    let isVideoOn = true;

    const hands = new Hands({
      locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`
    });
    hands.setOptions({
      maxNumHands: 1,
      modelComplexity: 1,
      minDetectionConfidence: 0.7,
      minTrackingConfidence: 0.7
    });

    hands.onResults((results) => {
      if (!model) return;

      const predictionBox = document.getElementById('prediction-box');
      if (results.multiHandLandmarks && results.multiHandLandmarks.length > 0) {
        const landmarks = results.multiHandLandmarks[0];
        const flat = landmarks.flatMap(pt => [pt.x, pt.y, pt.z]);

        const padded = flat.length < 188 ? flat.concat(new Array(188 - flat.length).fill(0)) : flat;
        const input = tf.tensor2d([padded]);
        const prediction = model.predict(input).dataSync();
        const maxIndex = prediction.indexOf(Math.max(...prediction));
        const confidence = prediction[maxIndex];
        predictionBox.innerText = confidence > confidenceThreshold ? `Sign: ${labels[maxIndex]}` : "Uncertain";
        tf.dispose(input);
      } else {
        predictionBox.innerText = "No Hand Detected";
      }
    });

    async function loadModel() {
      model = await tf.loadLayersModel("model_js/model.json");
      labels = await (await fetch("model_js/labels.json")).json();
    }

    async function joinCall() {
      await loadModel();
      client = AgoraRTC.createClient({ mode: "rtc", codec: "vp8" });
      await client.join(APP_ID, CHANNEL_NAME, TOKEN || null);

      localVideoTrack = await AgoraRTC.createCameraVideoTrack();
      localAudioTrack = await AgoraRTC.createMicrophoneAudioTrack();

      addUserStream("local-user", localVideoTrack, "You");
      await client.publish([localVideoTrack, localAudioTrack]);

      client.on("user-published", async (user, mediaType) => {
        await client.subscribe(user, mediaType);
        if (mediaType === "video") {
          addUserStream(user.uid, user.videoTrack);
        }
        if (mediaType === "audio") {
          user.audioTrack.play();
        }
      });

      client.on("user-unpublished", (user) => {
        removeUserStream(user.uid);
      });
    }

    function addUserStream(id, track, name = "Participant") {
      const container = document.createElement("div");
      container.className = "video-box";
      container.id = `user-${id}`;

      const video = document.createElement("video");
      video.autoplay = true;
      container.appendChild(video);
      track.play(video);

      const username = document.createElement("div");
      username.className = "username";
      username.innerText = name;
      container.appendChild(username);

      if (id === "local-user") {
        const predictionBox = document.createElement("div");
        predictionBox.className = "prediction-box";
        predictionBox.id = "prediction-box";
        predictionBox.innerText = "Loading AI...";
        container.appendChild(predictionBox);
        startGestureDetection(video);
      }

      document.getElementById("video-container").appendChild(container);
    }

    function removeUserStream(id) {
      const container = document.getElementById(`user-${id}`);
      if (container) container.remove();
    }

    function leaveCall() {
      if (localVideoTrack) localVideoTrack.stop();
      if (localAudioTrack) localAudioTrack.stop();
      if (camera) camera.stop();
      client.leave();
      document.getElementById("video-container").innerHTML = "";
    }

    function toggleMic() {
      if (!localAudioTrack) return;
      isMicOn = !isMicOn;
      localAudioTrack.setEnabled(isMicOn);
      document.getElementById("mic-icon").className = isMicOn ? "fas fa-microphone" : "fas fa-microphone-slash";
    }

    function toggleVideo() {
      if (!localVideoTrack) return;
      isVideoOn = !isVideoOn;
      localVideoTrack.setEnabled(isVideoOn);
      document.getElementById("video-icon").className = isVideoOn ? "fas fa-video" : "fas fa-video-slash";
    }

    function startGestureDetection(videoElement) {
      camera = new Camera(videoElement, {
        onFrame: async () => await hands.send({ image: videoElement }),
        width: 640,
        height: 480
      });
      camera.start();
    }
  </script>
</body>
</html>
