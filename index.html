<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Ivy Streams - Gesture Video Call</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://download.agora.io/sdk/release/AgoraRTC_N.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>

    <style>
        body {
            margin: 0;
            font-family: 'Segoe UI', sans-serif;
            background: linear-gradient(to right, #2C5364, #203A43, #0F2027);
            color: white;
        }

        h1 {
            text-align: center;
            margin-top: 20px;
            font-weight: 300;
        }

        #video-streams {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            margin: 20px auto;
            width: 90%;
            gap: 20px;
        }

        .video-box {
            position: relative;
            width: 45%;
            height: 45vh;
            background: #1e2b35;
            border-radius: 12px;
            overflow: hidden;
            border: 2px solid #4db8ff;
        }

        video {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        .prediction-box {
            position: absolute;
            bottom: 10px;
            left: 50%;
            transform: translateX(-50%);
            background-color: rgba(0, 0, 0, 0.7);
            padding: 8px 16px;
            border-radius: 6px;
            font-size: 16px;
        }

        #controls {
            text-align: center;
            margin: 20px;
        }

        button {
            padding: 12px 24px;
            margin: 0 10px;
            border: none;
            border-radius: 8px;
            background: #00bcd4;
            color: white;
            font-size: 16px;
            cursor: pointer;
            transition: background 0.3s;
        }

        button:hover {
            background: #0097a7;
        }

        @media (max-width: 768px) {
            .video-box {
                width: 90%;
                height: 40vh;
            }
        }
    </style>
</head>
<body>

<h1>Ivy Streams - Gesture Video Call</h1>

<div id="video-streams"></div>

<div id="controls">
    <button onclick="joinCall()">Join Call</button>
    <button onclick="leaveCall()">Leave Call</button>
</div>

<script>
    const APP_ID = "0f3fde8ae17c4048bcfc8d69286bc851";
    const CHANNEL_NAME = "test";
    const TOKEN = null;
    let client, localVideoTrack, localAudioTrack;
    let model;
    let labels = [];

    async function loadModel() {
        model = await tf.loadLayersModel('model_js/model.json');
        const res = await fetch('model_js/labels.json');
        labels = await res.json();
        console.log("Model and labels loaded.");
    }

    async function joinCall() {
        await loadModel();

        client = AgoraRTC.createClient({ mode: "rtc", codec: "vp8" });
        await client.join(APP_ID, CHANNEL_NAME, TOKEN, null);

        localVideoTrack = await AgoraRTC.createCameraVideoTrack();
        localAudioTrack = await AgoraRTC.createMicrophoneAudioTrack();

        addVideoElement("local-user", localVideoTrack);

        await client.publish([localVideoTrack, localAudioTrack]);

        client.on("user-published", async (user, mediaType) => {
            await client.subscribe(user, mediaType);
            if (mediaType === "video") {
                addVideoElement(user.uid, user.videoTrack);
            }
            if (mediaType === "audio") {
                user.audioTrack.play();
            }
        });

        client.on("user-unpublished", user => {
            removeVideoElement(user.uid);
        });
    }

    function leaveCall() {
        if (localVideoTrack) {
            localVideoTrack.stop();
            localVideoTrack.close();
        }
        if (localAudioTrack) {
            localAudioTrack.stop();
            localAudioTrack.close();
        }
        client.leave();
        document.getElementById("video-streams").innerHTML = "";
        console.log("Left the call");
    }

    function addVideoElement(userId, track) {
        const container = document.createElement("div");
        container.id = `user-${userId}`;
        container.className = "video-box";

        const videoElement = document.createElement("video");
        container.appendChild(videoElement);

        const predictionBox = document.createElement("div");
        predictionBox.className = "prediction-box";
        predictionBox.innerText = "Detecting...";
        container.appendChild(predictionBox);

        document.getElementById("video-streams").appendChild(container);
        track.play(videoElement);

        // Start predicting
        startGesturePrediction(videoElement, predictionBox);
    }

    function removeVideoElement(userId) {
        const el = document.getElementById(`user-${userId}`);
        if (el) el.remove();
    }

    async function startGesturePrediction(video, overlay) {
        const canvas = document.createElement("canvas");
        const ctx = canvas.getContext("2d");

        setInterval(() => {
            try {
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
                let imgData = tf.browser.fromPixels(canvas).resizeNearestNeighbor([14, 14]).toFloat();
                imgData = imgData.reshape([1, 188]);

                const prediction = model.predict(imgData);
                prediction.array().then(data => {
                    const maxProb = Math.max(...data[0]);
                    const index = data[0].indexOf(maxProb);
                    overlay.innerText = maxProb > 0.6 ? `Predicted: ${labels[index]}` : "Sign Not Recognized";
                });
            } catch (e) {
                console.log("Prediction error", e);
            }
        }, 1500);
    }
</script>
</body>
</html>
